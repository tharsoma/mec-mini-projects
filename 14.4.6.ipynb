{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdef0812-4fbe-4264-bddd-91ce7cde86d1",
   "metadata": {},
   "source": [
    "Text generation with an RNN\n",
    "https://www.tensorflow.org/text/tutorials/text_generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b2bf8ae-4252-470c-8fd7-e1ffa7fdf5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae0f3d08-125a-4e34-b4f4-4856e4aab257",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ceb37d5-8a7d-4b28-9e95-7beabcaa865c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "# Read, then decode for py2 compat.\n",
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "# length of text is the number of characters in it\n",
    "print(f'Length of text: {len(text)} characters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b15d7eb8-737c-4318-b7dd-9c6cb2fc54b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first 250 characters in text\n",
    "print(text[:250])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59739cc4-9f4f-42e2-b24f-22b30300cee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# The unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78f3536f-ca66-4b6f-83cc-efecf8bf05cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e6ac3f4-add5-48ab-93ff-77a7c5790cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d3c4493-4adc-4cb3-95c0-4ef11479b33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98d89124-432f-49e8-a46f-056c0567f132",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fbc9a6f-0751-40c3-a91f-921b924b563b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b26ecfb6-6186-4146-9e6a-700bf36cdb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "537f6f24-5913-432b-a6a9-a41853ca01a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30e0c469-b870-45e5-8399-5c9ff34c8877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c81b966c-23c3-4597-8213-7d850f552a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f39ddf5-3093-42e6-9d3c-0e3f95cd8d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f656923-5202-4767-8ba3-e51dfe6db415",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "examples_per_epoch = len(text)//(seq_length+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d90df362-c91b-4786-9d69-d152fcc3db82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47bc1c8b-f8ac-4236-8145-befadd80d374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a62fc838-59a8-4799-acdf-22b222045cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c9ea5dd-b526-4787-b8bc-1ebf0b3dc18c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
       " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_input_target(list(\"Tensorflow\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ddc7168-4476-42cc-b1af-0777d9da661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa3363bd-7dbe-4499-8942-dd18ddfdebb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abe9c66a-6685-4341-a377-4cdcd68b3a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1bb350cc-f390-44b4-aae6-9b6b1d3a1bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2d625140-1cef-4918-8c1b-fc6c5860f349",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b01782de-711d-467e-ba70-cb4be40df3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    # Be sure the vocabulary size matches the `StringLookup` layers.\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6693b93-bc5c-434a-bd0b-ec96c2751278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7121581-53cc-441d-9ee2-2e97740a3193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c32bac8-541d-495a-b119-42807e0766a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d1fc6ba-1ab9-4edf-a682-c9892021046e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 36, 60, 19,  7, 45, 45, 52, 63, 64,  7, 58, 36, 21, 53, 49, 52,\n",
       "        9, 53, 46, 25, 40, 15, 37, 60, 26, 48,  5, 30, 16,  9, 48, 28, 16,\n",
       "       44, 37, 15, 65, 46, 23,  7,  0, 22, 26, 10, 13, 25, 61, 37, 24, 34,\n",
       "       15,  4, 48, 25, 14, 37, 24, 38,  0,  4, 63, 43, 42,  1, 36,  3, 25,\n",
       "       62, 13, 58, 27, 43, 44, 55,  2, 40, 48, 48, 21, 65, 14, 31,  8, 30,\n",
       "       52, 34, 62, 25, 34, 51, 40, 35, 14, 25, 26, 10, 20, 31, 37],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "16c589de-5989-4274-8a5a-5eb3fbbb3273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'back to your native spring;\\nYour tributary drops belong to woe,\\nWhich you, mistaking, offer up to jo'\n",
      "\n",
      "Next Char Predictions:\n",
      " b'\\nWuF,ffmxy,sWHnjm.ngLaBXuMi&QC.iOCeXBzgJ,[UNK]IM3?LvXKUB$iLAXKY[UNK]$xdc\\nW!Lw?sNdep aiiHzAR-QmUwLUlaVALM3GRX'\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff943841-9f28-42d7-8897-b845e724f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fed5b96e-4153-4a3d-9ad4-c29bc1a81573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         4.1891766\n"
     ]
    }
   ],
   "source": [
    "example_batch_loss = loss(target_example_batch, example_batch_predictions)\n",
    "mean_loss = example_batch_loss.numpy().mean()\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", mean_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b4b9f263-127e-48a8-b7e0-fd9c8c4a37ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.968445"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(mean_loss).numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88b6277b-e342-46da-af77-22f37d7939ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "beae655f-47a3-4956-b608-68465a3201e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f40a189-a93d-4209-a9fe-194b8aaf8171",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "32a2c25d-cf4a-4938-9cd5-2033a2f52543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 181s 1s/step - loss: 2.7196\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 182s 1s/step - loss: 1.9888\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 178s 1s/step - loss: 1.7094\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 178s 1s/step - loss: 1.5460\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 179s 1s/step - loss: 1.4480\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 177s 1s/step - loss: 1.3804\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 177s 1s/step - loss: 1.3277\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 177s 1s/step - loss: 1.2840\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 180s 1s/step - loss: 1.2439\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 177s 1s/step - loss: 1.2039\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 177s 1s/step - loss: 1.1645\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 177s 1s/step - loss: 1.1238\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 176s 1s/step - loss: 1.0816\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 175s 1s/step - loss: 1.0363\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 176s 1s/step - loss: 0.9886\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 180s 1s/step - loss: 0.9390\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 181s 1s/step - loss: 0.8869\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 180s 1s/step - loss: 0.8359\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 182s 1s/step - loss: 0.7838\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 181s 1s/step - loss: 0.7356\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d2c85399-0fb8-4189-a086-c740b9b7a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f1bc2e9-1b4d-4665-b1c2-10994e42b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7dabe57f-44ae-40b2-9c4b-6107a0b24c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "The prince now fools.\n",
      "\n",
      "VINCENTIO:\n",
      "What he could never bear?\n",
      "\n",
      "MENENIUS:\n",
      "To move my sons so bright a gentleman there\n",
      "By the old princely Richmond, none but they should lay\n",
      "You die upon the heavy things transported\n",
      "And pall to scend a lame to acquaint her evil\n",
      "dares. If I do think, I get her go,\n",
      "I told me thy thousand wants of talk.\n",
      "\n",
      "ROMEO:\n",
      "It must go with me? we'll not dare not scapter'd. he is sit\n",
      "That makes me Aulike, for it is luck with\n",
      "Restinous too, to make them outcry.\n",
      "Accomples of the duke, even now to be?\n",
      "\n",
      "MENENIUS:\n",
      "Happy winter, thou art a traitor.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "BUCKINGHAM:\n",
      "Where is Lucentio?\n",
      "\n",
      "TRANIO:\n",
      "Well, sir, come you to all the mind to make\n",
      "thee dear merries that shall be deposed;\n",
      "And as the event of the throat, living touch\n",
      "Brother his nature with his neighbour,--new-repuing\n",
      "for the measure of his present pardness, father,\n",
      "If you say that I'll hear it.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "Not to my wife, yet I will make this knave\n",
      "Your king and Warwick, although\n",
      "At least a roven art to open\n",
      "T \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.1865882873535156\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f17177df-e5b9-4f0f-83b8-1968cdf29cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nThou art point your grace again.\\n\\nFirst Murderer:\\nWhat, are you mad?\\n\\nJULIET:\\nI will contemp to hand of wrathful iron arm.\\nCourt have you have comfort on his word:\\nAnd now, good Capeb, the poison of the good queen's head:\\nHad loves how you can witness to how her all was famished.\\n\\nGREEN:\\nNow Margare I dare not was more wealth,\\nThy barnen kindly greater, and sup of\\nyoung not reepetting, speak not apidect\\nHere in my knee to me, I'll bear myself all o'er!\\n\\nGREMIO:\\nCursed well! you are depays them.\\nHath he deliver'd to him.\\n\\nISABELLA:\\nAgainst all new of them over the more intent,\\nAnd after him, when it sound of pound.\\n\\nRICHARD:\\nI brought us what you please, not with the crown I mean to sleep,\\nAs thou wert cause a rap into a horseman's\\nfast, I bid them good.\\n\\nLUCIO:\\n'Tis fan abroad to sit upon thy head.\\nSweet Kate, we willingly giddy predotable,\\nTherefore, bedies out two climate. So, from thence will common\\nAbout the corsely plegge at me.\\nThese hollow mercy, which have way unto my knock,\\nT\"\n",
      " b\"ROMEO:\\nThe harder marches, what meets you hours--\\nThis lady have expressly chop O fine:\\nHad I not young away.\\n\\nCAMILLO:\\nIt is but yet most upright.\\n\\nROMEO:\\nNow, for my brother to be held you.\\n\\nBAPTISTA:\\nWhat, Juliet, Isabel, you will come.\\n\\nKING RICHARD III:\\nI gove the higher for vile-visabior.\\nThou broosh'd; when my blood is not hard, if sweat\\nShould with the father and thy colour lady\\nAmbits to cry to do with Japberous heir?\\nThither Somerset and thee not the commons.\\nIf I would fain, sir, at Rome should miss him,\\nGive some women arrived the crown, usban they\\nBecause his brother, take with gentle bringinch.\\nThere tyrrel get to our Bostol I: that lives I sell.\\nThen ladding hers our air.\\n\\nAUTOLYCUS:\\nA fooe of all that way.\\n\\nLEONTES:\\nNo, take too noble Percy.\\n\\nCORIOLANUS:\\nWhat, at thy cold concerts?\\n\\nKATHARINA:\\nHef thyself so-died; 'tis a lowly cap\\nAbove the people and thy party.\\n\\nQUEEN ELIZABETH:\\nGive me another deed.\\n\\nLORSS:\\nSignior Henry's tongue.\\n\\nKING RICHARD III:\\nSay that thy sudden band\"\n",
      " b\"ROMEO:\\nThou hast done a man of mine, I envy no more\\nThan can drink to close your honour:\\nI'll go and cry 'Charging your highness to our helmets,\\nWho do I hear, mard well prevented:\\nAnd with it be here with the ear of usin!\\nTake your mistress servingman: travellers, neighbour and a silly wing,\\nWhose househ, sir, here seem to that whippe\\nTunor--of a rague and high deliver,\\nSo bury may perceive your knees to give.\\nIn Grumio, no would effect the world's office\\nWould do thee good: my cellbains! a hateful duad?\\n\\nQUEEN ELIZABETH:\\nWhat then?\\n'Fore i' the lord to Sinisfer's sumbling? I'll bioly live.\\n\\nRICHARD:\\nGroyal ought low upon this woman:\\nthe dirstain of royal kings, fair woman,\\nIf they be so, discive wars\\nProvord thy crown before his friends.\\n\\nMERCUTIO:\\nIf it were dear to pierce it. First, how heaven he hath wings,\\nWhen Cliput speeds of his guilty shows have done, thou,\\nThe state, as they will kiss the traitor, where\\nshe for a prize more law, then leave you been.\\n\\nNORTHUMBERLAND:\\nThey will be h\"\n",
      " b\"ROMEO:\\nThen patience do I dry on sigh;\\nWe shall depose him a-damn'd sound.\\nAnd mourn give you no breath here to her love.\\n\\nKING RICHARD III:\\nYou have.\\n\\nLUCIO:\\nHere's no grave! thy all-resire hath look'd upon his tack from warm!\\nWelcome, Kate, let me tell you, think upon thee:\\nShow! what noise is this?\\nShall I be condemn'd to authoties but\\nwhich my soul trads and leaves that duty, fair within\\nA body, if any other smiles, hearing of thee,\\nHad-man than needs in brible, and witness of her medly.\\n\\nCORIOLANUS:\\nAy, to my embassage,\\nAnd learn to know that we drink a guest.\\n\\nLord Mayor:\\nMy liege, here's in our kinsman: Irmet\\nMiscorts me all the sunder for them, not\\nSignior I have been, may I shall die to-morrow\\nTo a wonder of the world, that you have been distance\\nMy swifter considerew boling upon thy pains\\nTo this same noble sell, as brief as though\\nThat would displease my sweat order humbily.\\n\\nGLOUCESTER:\\nI go; a man that villains, nothing of his friends:\\nMore gentlemen, but you have stay'd.\\n\\nTRANI\"\n",
      " b\"ROMEO:\\nGood night induced by my cold consent:\\nTo-day, we may, I'll spake, King Henry.\\n\\nKING EDWARD IV:\\nWhere is thy conscience still between us you have said,\\nStand bandy, to that word 'banished' to our enose\\nin: but wealing of mine woe's meaning, nor my rest; all dead\\nAs well a tentle youth, and so spoil succe departure\\nThy pures from his chambers knowledge in my stalls, coward\\nEven to the base hath been cancles not in a house woe\\nstucking onate, and are thou unfities fleet.\\nI was but welcome home to him!\\n\\nCAMILLO:\\nUnhup for the public hear,\\nHave token'd with no hall of tempest nose:\\nThe proudest of you to committe things\\nCan grieve not tyranny.\\n\\nSICINIUS:\\nIt is for it.\\nWhere is your brother?\\n\\nTRANIO:\\nIf he did hear them with his words.\\n\\nRIVERS:\\n\\nMIRANDA:\\nI doubt not much:\\nTo that which Shorts, his night's marriage, no more shoes to\\nPlash their heels without mine own.\\n\\nWARWICK:\\nCome, queens, you did give you the labourer:\\nIf ever you can witness to take home;\\nStrying Juliet; travellish you,\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 1.2662436962127686\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07c19da7-7cf9-479e-8747-d913df2012e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x0000022095BB6490>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1909dfc3-3147-49a8-84bf-556d3374ffac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "That we enjoy; for wrangling ityers course; and,\n",
      "Or be thyself good night, if thy will embrace a\n",
      "gr\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bd7152c7-fb44-4393-b0d0-3baad2dcfecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      inputs, labels = inputs\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(inputs, training=True)\n",
    "          loss = self.loss(labels, predictions)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      return {'loss': loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7a40cbe3-51cb-4948-a462-c3dad449c86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5badcc17-9241-4c8c-aceb-e0cbe1ad3b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e40c8efa-64b9-416f-a9cb-1050446d4472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 224s 1s/step - loss: 2.7508\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x220aa05b340>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe63d69e-1fd8-4086-baf2-71f66bd10d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.1993\n",
      "Epoch 1 Batch 50 Loss 2.0718\n",
      "Epoch 1 Batch 100 Loss 1.9917\n",
      "Epoch 1 Batch 150 Loss 1.8669\n",
      "\n",
      "Epoch 1 Loss: 2.0117\n",
      "Time taken for 1 epoch 240.62 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 2 Batch 0 Loss 1.8731\n",
      "Epoch 2 Batch 50 Loss 1.8376\n",
      "Epoch 2 Batch 100 Loss 1.6588\n",
      "Epoch 2 Batch 150 Loss 1.6457\n",
      "\n",
      "Epoch 2 Loss: 1.7293\n",
      "Time taken for 1 epoch 237.69 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 3 Batch 0 Loss 1.5924\n",
      "Epoch 3 Batch 50 Loss 1.5661\n",
      "Epoch 3 Batch 100 Loss 1.5551\n",
      "Epoch 3 Batch 150 Loss 1.5146\n",
      "\n",
      "Epoch 3 Loss: 1.5626\n",
      "Time taken for 1 epoch 235.59 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 4 Batch 0 Loss 1.4497\n",
      "Epoch 4 Batch 50 Loss 1.4843\n",
      "Epoch 4 Batch 100 Loss 1.4598\n",
      "Epoch 4 Batch 150 Loss 1.4344\n",
      "\n",
      "Epoch 4 Loss: 1.4606\n",
      "Time taken for 1 epoch 241.18 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 5 Batch 0 Loss 1.4127\n",
      "Epoch 5 Batch 50 Loss 1.3977\n",
      "Epoch 5 Batch 100 Loss 1.4117\n",
      "Epoch 5 Batch 150 Loss 1.3456\n",
      "\n",
      "Epoch 5 Loss: 1.3897\n",
      "Time taken for 1 epoch 248.75 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 6 Batch 0 Loss 1.3400\n",
      "Epoch 6 Batch 50 Loss 1.3716\n",
      "Epoch 6 Batch 100 Loss 1.3351\n",
      "Epoch 6 Batch 150 Loss 1.3382\n",
      "\n",
      "Epoch 6 Loss: 1.3383\n",
      "Time taken for 1 epoch 255.33 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 7 Batch 0 Loss 1.2467\n",
      "Epoch 7 Batch 50 Loss 1.2939\n",
      "Epoch 7 Batch 100 Loss 1.2745\n",
      "Epoch 7 Batch 150 Loss 1.2736\n",
      "\n",
      "Epoch 7 Loss: 1.2923\n",
      "Time taken for 1 epoch 242.39 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 8 Batch 0 Loss 1.2378\n",
      "Epoch 8 Batch 50 Loss 1.2847\n",
      "Epoch 8 Batch 100 Loss 1.2426\n",
      "Epoch 8 Batch 150 Loss 1.2541\n",
      "\n",
      "Epoch 8 Loss: 1.2518\n",
      "Time taken for 1 epoch 222.36 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 9 Batch 0 Loss 1.2127\n",
      "Epoch 9 Batch 50 Loss 1.2066\n",
      "Epoch 9 Batch 100 Loss 1.1737\n",
      "Epoch 9 Batch 150 Loss 1.2165\n",
      "\n",
      "Epoch 9 Loss: 1.2133\n",
      "Time taken for 1 epoch 235.32 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 10 Batch 0 Loss 1.1526\n",
      "Epoch 10 Batch 50 Loss 1.1227\n",
      "Epoch 10 Batch 100 Loss 1.1904\n",
      "Epoch 10 Batch 150 Loss 1.2098\n",
      "\n",
      "Epoch 10 Loss: 1.1746\n",
      "Time taken for 1 epoch 240.61 sec\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "mean = tf.metrics.Mean()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    mean.reset_states()\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        logs = model.train_step([inp, target])\n",
    "        mean.update_state(logs['loss'])\n",
    "\n",
    "        if batch_n % 50 == 0:\n",
    "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
    "            print(template)\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print()\n",
    "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
    "    print(\"_\"*80)\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599940c0-6b58-413d-b7e6-d576ed1c7aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
